{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations of Machine Learning \n",
    "### Assignment 1\n",
    "###### Submitted by - \n",
    "Sakshi Badole\n",
    "CS24MTECH11008"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Pandas and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading data from dataset into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "red_df = pd.read_csv('wine+quality/winequality-red.csv', sep=';') \n",
    "white_df = pd.read_csv('wine+quality/winequality-white.csv', sep=';') \n",
    "\n",
    "#merge white and red wine data files\n",
    "data = red_df.merge(white_df, how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train-Test-Split: Coded from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test-split\n",
    "\n",
    "indices_array = np.array(data.index.tolist())\n",
    "test_size = int(0.2*len(data))\n",
    "\n",
    "# randomly assigns 20 percent of the indices as the test indices\n",
    "test_indices = indices_array[np.random.choice(indices_array.shape[0], test_size, replace=False)]\n",
    "\n",
    "# test data for that the randomly picked indices \n",
    "test_df = data.loc[test_indices]\n",
    "\n",
    "# train dataframe created by dropping the test indices\n",
    "train_df = data.drop(test_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe to 2D array using .values function\n",
    "\n",
    "X_train = train_df.drop('quality', axis=1).values\n",
    "\n",
    "X_test = test_df.drop('quality', axis=1).values\n",
    "# train and test label(quality) being assigned \n",
    "y_train = train_df['quality']\n",
    "y_test = test_df['quality']\n",
    "\n",
    "# flatten 2D array to 1D array \n",
    "y_data = y_train.values.flatten()\n",
    "# Assigning new class values i.e 0 for quality values under 7 and 1 for quality values >= 7\n",
    "y_data = np.where(y_data >= 7 , 1, 0)\n",
    "y_test = y_test.values.flatten()\n",
    "y_test = np.where(y_test >= 7 , 1, 0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Information Gain Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_information_gain(X, y, feature_idx, limit, mode='entropy'):\n",
    "    #parent and weighted child entropy is being calculated for the given feature index\n",
    "    # returns the information gain i.e. parent - weighted child entropy\n",
    "    if mode == 'gini':\n",
    "        parent_gini = calculate_gini(y)\n",
    "        left_y = y[limit >= X[:, feature_idx] ]\n",
    "        right_y = y[limit < X[:, feature_idx] ]\n",
    "        left_gini = calculate_gini(left_y)\n",
    "        right_gini = calculate_gini(right_y)\n",
    "        return parent_gini - (len(left_y) / len(y))*left_gini + (len(right_y) / len(y))*right_gini\n",
    "    else:\n",
    "        parent_entropy = calculate_entropy(y)\n",
    "        left_y = y[limit >= X[:, feature_idx] ]\n",
    "        right_y = y[limit < X[:, feature_idx] ]\n",
    "        left_entropy = calculate_entropy(left_y)\n",
    "        right_entropy = calculate_entropy(right_y)\n",
    "        child_entropy = (len(left_y) / len(y)) * left_entropy + (len(right_y) / len(y)) * right_entropy\n",
    "        return parent_entropy - child_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entropy Calculation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(y):\n",
    "    # calculates the entropy of the entered subtree/tree\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    sum = 0\n",
    "    y_prob = counts / len(y)\n",
    "\n",
    "    for p in y_prob:\n",
    "        if p > 0:\n",
    "            sum = sum + p * np.log2(p)\n",
    "    # returns the entropy value using the formula -Summation(p*log(p))\n",
    "    # here p is the probability of a sample of that class\n",
    "    return -sum\n",
    "    # return -np.sum([p * np.log2(p) for p in y_prob if p>0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gini Index (1-Summation(p^2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(y):\n",
    "    unique, counts = np.unique(y, return_counts =True)\n",
    "    sum=0\n",
    "    y_prob = counts / len(y)\n",
    "    for p in y_prob:\n",
    "        sum = sum + p**2\n",
    "    return ( 1 - sum )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, y_train, max_depth=None, min_samples_split=2, mode='entropy'):\n",
    "    \n",
    "    \n",
    "    if max_depth == 0 or len(np.unique(y_train)) == 1 or len(y_train) < min_samples_split:\n",
    "        return {'leaf': True, 'class': np.argmax(np.bincount(y_train))}\n",
    "    \n",
    "    samples, feat_index = X_train.shape\n",
    "    all_features = X_train.shape[1]\n",
    "\n",
    "    feature_indexes = np.random.choice(feat_index, all_features, replace=False)\n",
    "\n",
    "    best_feature = None\n",
    "    best_value = None\n",
    "    best_gain = 0\n",
    "\n",
    "    for feature_idx in feature_indexes:\n",
    "        unique_values = np.unique(X_train[:, feature_idx])\n",
    "        for limit in unique_values:\n",
    "            gain = calculate_information_gain(X_train, y_train, feature_idx, limit, mode)\n",
    "            if best_gain < gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature_idx\n",
    "                best_value = limit\n",
    "    \n",
    "    if best_gain == 0:\n",
    "        return {'leaf': True, 'class': np.argmax(np.bincount(y_train))}\n",
    "    \n",
    "    left_X, left_y = X_train[X_train[:, best_feature] <= best_value], y_train[X_train[:, best_feature] <= best_value ]\n",
    "    right_X, right_y = X_train[X_train[:, best_feature] > best_value ], y_train[X_train[:, best_feature] > best_value ]\n",
    "    \n",
    "    left_tree = decision_tree(left_X, left_y,  max_depth-1, min_samples_split)\n",
    "    right_tree = decision_tree(right_X, right_y, max_depth-1, min_samples_split)\n",
    "\n",
    "    return {'leaf': False, 'left': left_tree, 'right': right_tree, 'feature': best_feature, 'limit': best_value}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Traverses through each node to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, X):\n",
    "    #fills in zero value for the entirety of samples\n",
    "    predictions = np.ones(X.shape[0])\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        node = tree\n",
    "        while not node['leaf']:\n",
    "            feature_idx = node['feature']\n",
    "            if X[i, feature_idx] <= node['limit']:\n",
    "                node = node['left']\n",
    "            else:\n",
    "                node = node['right']\n",
    "        predictions[i] = node['class']\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to post-prune the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_prune(tree, X_test, y_test):\n",
    "    \n",
    "    if tree['leaf']:\n",
    "        return tree\n",
    "\n",
    "    \n",
    "    left_tree = post_prune(tree['left'], X_test, y_test)\n",
    "    right_tree = post_prune(tree['right'], X_test, y_test)\n",
    "\n",
    "    \n",
    "    if left_tree['leaf'] and right_tree['leaf']:\n",
    "        # Calculate the error of the current node\n",
    "        predictions = predict(tree, X_test)\n",
    "        error = np.mean(predictions != y_test)\n",
    "\n",
    "        left_predictions = predict(left_tree, X_test)\n",
    "        left_error = np.mean(left_predictions != y_test)\n",
    "        right_predictions = predict(right_tree, X_test)\n",
    "        right_error = np.mean(right_predictions != y_test)\n",
    "\n",
    "        \n",
    "        if error - (left_error + right_error) / 2 < 0.01:\n",
    "            return {'leaf': True, 'class': np.argmax(np.bincount(y_test))}\n",
    "\n",
    "    \n",
    "    return {'leaf': False, 'left': left_tree, 'right': right_tree, 'feature': tree['feature'], 'limit': tree['limit']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = decision_tree(X_train, y_data, max_depth=5, mode='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pruned = post_prune(tree, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_predictions = predict(tree, X_test)\n",
    "y_predictions_afterpruning = predict(tree_pruned, X_test)\n",
    "print(y_predictions)\n",
    "print(y_predictions_afterpruning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test, predictions):\n",
    "    return np.sum(y_test == predictions) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without pruning  0.8421862971516552\n",
      "Accuracy with Pruning  0.8344880677444187\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy without pruning \",accuracy(y_test, y_predictions))\n",
    "print(\"Accuracy with Pruning \",accuracy(y_test, y_predictions_afterpruning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating train and test dataset from the data(wine-quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6495, 11)\n",
      "(6495,)\n",
      "[0 1 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "#taking k = 10\n",
    "k = 10\n",
    "\n",
    "data_X = data.drop('quality', axis=1).values\n",
    "data_y = data['quality'].values.flatten()\n",
    "data_y = np.where(data_y >= 7 , 1, 0)\n",
    "print(data_X.shape)\n",
    "print(data_y.shape)\n",
    "print(data_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the entire dataset in k sets, creating a list of lists(containing values for certain indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size, features = data_X.shape\n",
    "new_size = round(sample_size/k)\n",
    "\n",
    "data_Xi = list()\n",
    "data_yi = list()\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    if i == k-1:\n",
    "        X = data_X[(k-1)*new_size: sample_size, :]\n",
    "        y = data_y[(k-1)*new_size: sample_size]\n",
    "        data_Xi.append(X.tolist())\n",
    "        data_yi.append(y.tolist())\n",
    "\n",
    "\n",
    "    else:\n",
    "        X = data_X[ i*new_size : (i+1)*new_size , : ]\n",
    "        y = data_y[ i*new_size : (i+1)*new_size ]\n",
    "        data_Xi.append(X.tolist())\n",
    "        data_yi.append(y.tolist())\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calling Decision tree for k fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating X_train_new and y_train_new by merging k-1 datasets and calling decision tree for this new dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking predictions for X_test_new using y_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the iteration 0 is:  0.7276923076923076\n",
      "Accuracy for the iteration 1 is:  0.8\n",
      "Accuracy for the iteration 2 is:  0.8092307692307692\n",
      "Accuracy for the iteration 3 is:  0.8076923076923077\n",
      "Accuracy for the iteration 4 is:  0.816923076923077\n",
      "Accuracy for the iteration 5 is:  0.8076923076923077\n",
      "Accuracy for the iteration 6 is:  0.8353846153846154\n",
      "Accuracy for the iteration 7 is:  0.8876923076923077\n",
      "Accuracy for the iteration 8 is:  0.8584615384615385\n",
      "Accuracy for the iteration 9 is:  0.8217054263565892\n"
     ]
    }
   ],
   "source": [
    "accuracy_sum = 0\n",
    "\n",
    "X_trainf = []\n",
    "X_testf = []\n",
    "y_trainf = []\n",
    "y_testf = []\n",
    "\n",
    "for i in range(k):\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    \n",
    "    for j in range(k):\n",
    "        if j == i:\n",
    "            X_test.extend(data_Xi[j])\n",
    "            y_test.extend(data_yi[j])\n",
    "        else:\n",
    "            X_train.extend(data_Xi[j])\n",
    "            y_train.extend(data_yi[j])\n",
    "    \n",
    "    X_testf.append(X_test)\n",
    "    y_testf.append(y_test)\n",
    "    X_trainf.append(X_train)\n",
    "    y_trainf.append(y_train)\n",
    "\n",
    "    \n",
    "for i in range(k):\n",
    "    X_train_new = np.array(X_trainf[i])\n",
    "    X_test_new = np.array(X_testf[i])\n",
    "    y_train_new = np.array(y_trainf[i])\n",
    "    y_test_new = np.array(y_testf[i])\n",
    "\n",
    "\n",
    "    #Decision Tree Call\n",
    "    tree = decision_tree(X_train_new, y_train_new, max_depth=5)\n",
    "    predictions = predict(tree, X_test_new)\n",
    "    data_acc = accuracy(y_test_new, predictions)\n",
    "    print(\"Accuracy for the iteration\",i, \"is: \",data_acc)\n",
    "    accuracy_sum += data_acc\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree with k-fold Cross Validation with k=10: 0.817247465712582\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the Decision Tree with k-fold Cross Validation with k=10:\", accuracy_sum/k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
